{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e34959-886a-4e57-a62b-eea8ae963508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fastai\n",
      "Version: 2.7.10\n",
      "Summary: fastai simplifies training fast and accurate neural nets using modern best practices\n",
      "Home-page: https://github.com/fastai/fastai\n",
      "Author: Jeremy Howard, Sylvain Gugger, and contributors\n",
      "Author-email: info@fast.ai\n",
      "License: Apache Software License 2.0\n",
      "Location: /Users/gwilliams/Projects/SEEMAP2023/venv/seemap2023/lib/python3.8/site-packages\n",
      "Requires: fastcore, fastdownload, fastprogress, matplotlib, packaging, pandas, pillow, pip, pyyaml, requests, scikit-learn, scipy, spacy, torch, torchvision\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip show fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393151fc-4930-4482-bc1a-b7a98a60e8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())\n",
    "torch.device(\"mps\")\n",
    "\n",
    "from fastai.vision.all import *\n",
    "default_device(torch.device(\"mps\"))\n",
    "\n",
    "from timm import create_model\n",
    "import timm\n",
    "\n",
    "from fastai.learner import Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0048b412-2936-4515-b6d3-570f6787fda7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    #timm.list_models(pretrained=False)\n",
    "\n",
    "    modelstrs = timm.list_models(pretrained=False)\n",
    "    for modelstr in modelstrs:\n",
    "        if modelstr.find(\"mobilenet\")>=0:\n",
    "            body = create_timm_body(modelstr, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "060a1cba-f1bf-4644-98b0-9e9d45155fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gwilliams/.fastai/data/mnist_png\n"
     ]
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f592c7-97b4-488e-afef-62dc34c399b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "block = DataBlock(\n",
    "        blocks=(ImageBlock, CategoryBlock),\n",
    "        get_items=get_image_files,\n",
    "        splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "        get_y=parent_label)\n",
    "#        batch_tfms=aug_transforms(mult=2., do_flip=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7c5967-3912-4809-9e1a-e945529ac093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaders = block.dataloaders(path/\"training\")\n",
    "#loaders.train.show_batch(max_n=4, nrows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c3b04a2-20ef-42e4-a3a0-6930a8ac9f96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAD0CAYAAACvgrpiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAVWklEQVR4nO3dfZCVZf0/8OsAWQRiw2ARySITasVDaTgJM7QLM0ZMQzG1LJY2IMMfCEIm/qEJsaKQzgjiQ0lNhs3Yw7DYg9lYZLFJyDqV0oTVQE3JsoRJYDwk8nR+f3x/X/tmXfc5e3bP3ufafb1m/Gffe9/3hyMXzNsb+RSKxWIxAAAAQKL65T0AAAAAdIViCwAAQNIUWwAAAJKm2AIAAJA0xRYAAICkKbYAAAAkTbEFAAAgaYotAAAASVNsAQAASJpiCwAAQNIU28StXr06FAqFMG7cuLxHAcp07NixsHLlyvDhD384DB06NBQKhfDwww/nPRZQgWeffTZ89KMfDUOHDg1vfvObw7hx48J9992X91hAmX75y1+G66+/PowdOzYMGjQo1NXVhaamprB79+68R6OTBuQ9AJXbt29fWLNmTRg0aFDeowCdcPDgwbBq1apQV1cX3vve94bW1ta8RwIqsGXLljBz5sxw6aWXhhUrVoTBgweHP/3pT2Hfvn15jwaU6a677grbt28Ps2fPDhMmTAgHDhwIDzzwQLjssstCW1ubl0cJKRSLxWLeQ1CZq666Krz00kvhzJkz4eDBg2HXrl15jwSU4dVXXw2HDx8Ow4cPD7/61a/C5ZdfHjZu3BjmzZuX92hAmY4cORIuvvjiMHny5LB58+bQr58/BAcpevrpp8PEiRPDOeec89rX9uzZE8aPHx8aGxvDI488kuN0dIZfhRP11FNPhc2bN4f169fnPQrQSW984xvD8OHD8x4D6IJvfvOb4cUXXwyrV68O/fr1C8ePHw9nz57NeyygkyZPnvxvpTaEEC666KIwduzY8Pvf/z6nqaiEYpugM2fOhCVLloQFCxaE8ePH5z0OAPQ5Tz75ZBgyZEjo6OgIl1xySRg8eHAYMmRIuO6668KJEyfyHg/ogmKxGF588cUwbNiwvEehExTbBG3YsCG88MIL4fbbb897FADok/bs2RNOnz4dPvaxj4Xp06eHRx99NMyfPz9s2LAhXHvttXmPB3TBN77xjdDR0RHmzJmT9yh0gr88KjF///vfw+c///mwYsWKcP755+c9DgD0SceOHQv//Oc/w8KFC1/7W5A//vGPh5MnT4Yvf/nLYdWqVeGiiy7KeUqgs/7whz+ExYsXh0mTJoW5c+fmPQ6d4I1tYpYvXx6GDh0alixZkvcoANBnDRw4MIQQwic/+cl/+/qnPvWpEEIIO3bs6PGZgK45cOBA+MhHPhLOO++8sHnz5tC/f/+8R6ITvLFNyJ49e8JXvvKVsH79+rB///7Xvn7ixIlw6tSp8Je//CUMGTIkDB06NMcpAaD3GzFiRHj++efD2972tn/7+lvf+tYQQgiHDx/OYyygQv/4xz/CjBkzwssvvxy2bdsWRowYkfdIdJI3tgnp6OgIZ8+eDUuXLg2jR49+7Z9nnnkm7N69O4wePTqsWrUq7zEBoNd7//vfH0L4n9+b/6///Q/P/nchSMeJEyfCzJkzw+7du8Pjjz8e3vOe9+Q9EhXwxjYh48aNC9/97nf/4+vLly8PR48eDffee2945zvfmcNkANC3NDU1hTvvvDM89NBDYdq0aa99/atf/WoYMGBAaGhoyG84oGxnzpwJc+bMCTt27Ajf//73w6RJk/IeiQoptgkZNmxYmDVr1n98/X932f63DKhNDzzwQHj55Zdfe7vzgx/8IOzbty+EEMKSJUvCeeedl+d4QAmXXnppmD9/fvja174WTp8+Herr60Nra2toaWkJt9xyiz/GCIlYtmxZeOyxx8LMmTPDoUOHwiOPPPJv+TXXXJPTZHRWoVgsFvMegq5paGgIBw8eDLt27cp7FKBMF154YXjhhRf+a/bnP/85XHjhhT07ENBpp06dCmvWrAkbN24M+/fvD6NGjQqLFy8ON9xwQ96jAWVqaGgIP//5z6O5qpQOxRYAAICk+cujAAAASJpiCwAAQNIUWwAAAJKm2AIAAJA0xRYAAICkKbYAAAAkTbEFAAAgaQPK/cZCoVDNOaDXqPXV0M4ylKeWz7JzDOWp5XMcgrMM5SrnLHtjCwAAQNIUWwAAAJKm2AIAAJA0xRYAAICkKbYAAAAkTbEFAAAgaYotAAAASVNsAQAASJpiCwAAQNIUWwAAAJKm2AIAAJA0xRYAAICkKbYAAAAkTbEFAAAgaYotAAAASVNsAQAASJpiCwAAQNIUWwAAAJKm2AIAAJA0xRYAAICkDch7AAAgbePGjYtmTzzxROa1I0aMiGZ33nlnNLv11ltLDwZAn+GNLQAAAElTbAEAAEiaYgsAAEDSFFsAAACSptgCAACQNMUWAACApFn3AwB0yZQpU6LZ29/+9sxri8ViRRkA/F/e2AIAAJA0xRYAAICkKbYAAAAkTbEFAAAgaYotAAAASVNsAQAASJp1PwA17nOf+1w0u+OOOyq65+9+97vM/Morr4xmf/3rXyt6Jr3XvHnz8h4BoFs0NzdX5b6tra0VZZTPG1sAAACSptgCAACQNMUWAACApCm2AAAAJE2xBQAAIGmKLQAAAEmz7gegxhWLxYqyLO9+97sz8wsuuCCaWffD602cODGaVfpzFKCUrNU89fX10ayhoaH7hylh5cqV0azUup+pU6d28zS9kze2AAAAJE2xBQAAIGmKLQAAAElTbAEAAEiaYgsAAEDSFFsAAACSptgCAACQNHtse8jatWujWUdHRzRbt25dNcbJxdNPPx3NJk+e3IOTQFpuvvnmbr/nmjVrMvPf/OY33f5Meq9du3ZFs7Fjx1Z83zFjxkSzgQMHRrNXXnml4mfm4cc//nE0u/jiiyu+74oVK6LZ/v37o9nPfvazip8J/03W3tisLGv3a29Saq/u1q1bo5kdt//ijS0AAABJU2wBAABImmILAABA0hRbAAAAkqbYAgAAkDTFFgAAgKQVisVisaxvLBSqPUvSRo4cmZnv3bs3mmWt9Fm2bFnFM+Whqakpmt19993RrK6urhrj5KLMI5UbZ7n2lFqH8tvf/jaaZf18y1rpc/vtt2c+8+TJk5l5X1DLZ7nWzvF1110Xze6///6qPHP06NHRrL29vSrP7IprrrkmmmV9Rueee241xslciXTttddGs82bN1djnKqp5XMcQu2d5a5obm6OZnms7WltbY1meazIqdbnk/VjyfoMUlPOWfbGFgAAgKQptgAAACRNsQUAACBpii0AAABJU2wBAABImmILAABA0gbkPUBvMWnSpMy8paUlmqW20idLY2NjNMv6DKC3GzRoUDTbsmVLVZ754IMPRjPrfKB7zZo1KzP/+te/Hs3yWEkzcODAaJa1vunxxx/PvO+JEycqnonatnXr1sy8oaGh25952223ZeZZ62xqbdVN1jxdWfeT9bnX2mdQbd7YAgAAkDTFFgAAgKQptgAAACRNsQUAACBpii0AAABJU2wBAABImnU/3SRrzU0IIbS1tfXQJNVVaq3R7Nmzo1ldXV13jwPJ6N+/fzQbPnx45rWFQiGa5bEmBHrKPffcE806Ojp6cJLSrrzyyqrc98knn4xmTU1NmddOnz49mn3rW9+KZvX19dHssccey3zm1VdfHc1eeumlzGvJX3NzczTryjqfrLUzWSt9etO6mq6sJqrGKqXeyBtbAAAAkqbYAgAAkDTFFgAAgKQptgAAACRNsQUAACBpii0AAABJU2wBAABImj22nZC1w/WKK67IvLbUrrlKdOWemzZtqui6UntsW1paoll7e3tFz4Te4Pzzz6/KfQ8fPhzNTp8+XZVnQk959dVXo9nZs2d7cJLqOn78eDS79957o9mRI0cy7/vTn/40mj366KPR7BOf+EQ0mzZtWuYzH3zwwWjW2NiYeS35W7lyZTQrtWu1r+yjpXZ5YwsAAEDSFFsAAACSptgCAACQNMUWAACApCm2AAAAJE2xBQAAIGnW/XRC1qqbrDU3IYQwcuTIaLZ27dpoNnv27NKDVSDrr9xftmxZNLvhhhsy79vW1lbpSJC8/v37R7Nbb7214vsWi8VotmHDhmj2t7/9reJnQi2YPn16NMv6vTNrDVYteu6556LZE088UfF9Dx06FM0WLFgQzc4999xo9qEPfSjzmVOmTKko27ZtW+Z96RldWdljpU+2hoaGijLK540tAAAASVNsAQAASJpiCwAAQNIUWwAAAJKm2AIAAJA0xRYAAICkWffzOllreUqtusly4403RrOsVUFz5syJZu3t7RXP89nPfjaabd++veL7WvdDXzZq1KhoNnfu3Irvu2XLlmi2atWqiu8L3aVQKFSUlXLZZZdFs8GDB0ezaq37mTBhQjRbuHBh5rX9+sXfJVx//fUVz1Spo0ePRrOdO3dGs6wVTCGEMGzYsGiWtQ4ma10aPae5uTnvEXqtaq30sWbpX7yxBQAAIGmKLQAAAElTbAEAAEiaYgsAAEDSFFsAAACSptgCAACQNOt+XmfSpEnRLGsVUKnVO1lrezZt2lR6sG62b9++aLZ3795otmPHjsz7rlu3ruKZIHUrV66syn2fffbZaHby5MmqPBM649vf/nY0W7p0aea1Y8aMqeiZl19+eTQ7fvx45rWHDh2q6JnFYrGiLIQQzp49W9Ez83DfffdFs0WLFmVem7WGCfqy+vr6iq/NWulj3c+/eGMLAABA0hRbAAAAkqbYAgAAkDTFFgAAgKQptgAAACRNsQUAACBpii0AAABJs8f2de6+++5olrXDddmyZZn3LbX/taddcMEFFV23fv367h0E+ohCoVDxtU899VQ3TgLdL2sv7Be/+MXMa++6665ods4550SzrB3w3/nOdzKf2dTUlJnHzJs3r6LruiLr144hQ4ZkXnv06NFolrVX98CBA9HsoYceynzmZz7zmcwcOqO5uTnvEbpNQ0NDVe5ba59RnvN4YwsAAEDSFFsAAACSptgCAACQNMUWAACApCm2AAAAJE2xBQAAIGl9ct3PjTfeGM1GjhwZzW666aZoVmvrfErJ+nG2tLREs6z1CtAXXHLJJdHsqquuimbFYjGalfr140c/+lHpwaBG3X///Zn5Bz7wgWiWdaayTJs2LTO/+eabo9k999wTzQ4fPlzRPKXMnz8/mh05ciSaLV++PPO+o0ePjmbt7e2lB/svfv3rX1d0HX1b1gqY+vr6aFatFTmpyfocau0zsu4HAAAAKqTYAgAAkDTFFgAAgKQptgAAACRNsQUAACBpii0AAABJ65XrfrJW2YQQwtq1a6PZunXrollvWnXT2NgYzdra2npwEkjLLbfcEs0GDIj/knrs2LFoNmPGjC7NBCnLWr3zvve9L5q9613vimZvectbMp95xx13RLOJEydGs8WLF0ezMWPGZD7z05/+dDRbunRpNDt16lQ0++EPf5j5zFdeeSUzr8TYsWO7/Z6kodRama1bt/bMIGVqbW2NZrW2Iic1WZ9tnryxBQAAIGmKLQAAAElTbAEAAEiaYgsAAEDSFFsAAACSptgCAACQtEKxWCyW9Y2FQrVn6Tal1v3Mnj07mmWt+0lN1uewd+/eaFZXVxfN2tvbuzRTX1DmkcpNSmc5DxMmTMjMt23bFs0GDx4czY4ePRrNSq0mIR+1fJb7yjletGhRNFu9enU0y1q9FUIIAwcOrGieHTt2RLN3vOMdmdeOGjUqmu3fvz+afelLX4pmX/jCFzKfWal58+ZFsw0bNmReW+qz7+7rSqnlcxxCWme51DqfSlfo3HbbbdGsubm5onuWUq2fF3n8WPqKcv6deWMLAABA0hRbAAAAkqbYAgAAkDTFFgAAgKQptgAAACRNsQUAACBpii0AAABJq87SsJyV2rXam3bVZpk0aVI0a2lpiWZ21dKXvelNb8rMs3bVZvnjH/9Y0XXQl2XtcM06U1n72EMovYs1Juv31a6YMWNGNNu1a1dVnpm1P3vBggXRrCv7Zr/3ve9VfC09I2vXaqV7akOo3n7XrJlWrlxZ8X2zTJ06NZq1trZW5ZmUxxtbAAAAkqbYAgAAkDTFFgAAgKQptgAAACRNsQUAACBpii0AAABJ65XrfvgfjY2N0Wzz5s09OAmkY+TIkRVfe/DgwWg2a9asiu8L/KctW7ZEs2HDhmVe+8EPfjCazZ49O5q94Q1vKD1YBX7xi19Es7Nnz1blmf36xd9tVLrWLIQQHn744Wi2aNGiiu9Lz8haV9OV9Tn19fXRLGvdT7VW9mStHyq1ssdKn9rljS0AAABJU2wBAABImmILAABA0hRbAAAAkqbYAgAAkDTFFgAAgKQVisVisaxvLBSqPQudVGotyfbt26NZXV1dd4/D/1fmkcqNs5ztJz/5SWY+bdq0aNbR0RHNnLn01PJZdo6r54orrohmN910UzSbMmVK5n2zVhDV8s+113v++ecz87lz50aznTt3dvM0pdX6Z5vSWc5ayxNC9VbzZMlavTN16tSeG4SqK+cse2MLAABA0hRbAAAAkqbYAgAAkDTFFgAAgKQptgAAACRNsQUAACBpA/IegMq1t7dn5qXWAQHdq6WlJe8RgC5qa2uLZo2NjdGs1LqfhQsXRrM5c+aUHqybHTp0KJqtWLEimm3cuDHzvidPnqx4JmpbqXU/veWZpMsbWwAAAJKm2AIAAJA0xRYAAICkKbYAAAAkTbEFAAAgaYotAAAASVNsAQAASJo9tr2YnZrQs3bu3Jn3CEBOtm3bVnF+9dVXd/c40OPsnCVv3tgCAACQNMUWAACApCm2AAAAJE2xBQAAIGmKLQAAAElTbAEAAEiadT8Ja2pqyszb2tp6aBLoPbZs2ZKZjx8/Ppo999xz3T0OAABl8MYWAACApCm2AAAAJE2xBQAAIGmKLQAAAElTbAEAAEiaYgsAAEDSCsVisVjWNxYK1Z6FTiq17qexsbGia7OyHTt2ZD6zvb09M+8LyjxSuXGWoTy1fJadYyhPLZ/jEJxlKFc5Z9kbWwAAAJKm2AIAAJA0xRYAAICkKbYAAAAkTbEFAAAgaYotAAAASbPuJ2EjR47MzPfu3RvN1q1bF82eeeaZaLZp06bSg/VxVgtA71DLZ9k5hvLU8jkOwVmGcln3AwAAQK+n2AIAAJA0xRYAAICkKbYAAAAkTbEFAAAgaYotAAAASVNsAQAASJo9ttDN7MyD3qGWz7JzDOWp5XMcgrMM5bLHFgAAgF5PsQUAACBpii0AAABJU2wBAABImmILAABA0hRbAAAAklb2uh8AAACoRd7YAgAAkDTFFgAAgKQptgAAACRNsQUAACBpii0AAABJU2wBAABImmILAABA0hRbAAAAkqbYAgAAkLT/B4jnyEXq24/RAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1200x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaders.train.show_batch(max_n=4, nrows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2054a068-29a3-4954-8c23-19766989509b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# squeezenet default fast ai learner\n",
    "#learn = vision_learner(loaders, squeezenet1_0, loss_func=nn.CrossEntropyLoss(), metrics=accuracy)\n",
    "#learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836a7f2-44b5-490b-bf33-3d7f708dd862",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Mnist_NN(nn.Module):\n",
    "    def __init__(self,pretrained):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(784, 512, bias=True) \n",
    "        self.lin2 = nn.Linear(512, 256, bias=True)\n",
    "        self.lin3 = nn.Linear(256, 10, bias=True)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        x = xb.view(-1,784) \n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        return self.lin3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f2df2-9e3d-4529-8a42-9c2afb45897a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_model(model, pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"model: function to load the model, e.g. resnet18\n",
    "        pretrained, progress: to be passed to the model function\n",
    "    \"\"\"\n",
    "    m = model(pretrained=pretrained, progress=progress, **kwargs) # loads standard model\n",
    "    ##m.avgpool = nn.AdaptiveAvgPool2d(output_size=(100,100)) # changes one layer\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81510e4b-1114-4952-86a0-ae8603fd1cde",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from fastai.vision.learner import _update_first_layer\n",
    "def create_timm_body(arch:str, pretrained=True, cut=None, n_in=3):\n",
    "    \"Creates a body from any model in the `timm` library.\"\n",
    "    try:\n",
    "        model = create_model(arch, pretrained=pretrained, num_classes=0, global_pool='')\n",
    "        print(arch, type(model),sum([p.numel() for p in model.parameters()]))\n",
    "    \n",
    "        _update_first_layer(model, n_in, pretrained)\n",
    "        if cut is None:\n",
    "            ll = list(enumerate(model.children()))\n",
    "            cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n",
    "        if isinstance(cut, int): return nn.Sequential(*list(model.children())[:cut])\n",
    "        elif callable(cut): return cut(model)\n",
    "        else: raise NamedError(\"cut must be either integer or function\")\n",
    "    except:\n",
    "        print(\"error processing cut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d7ba1-f5ef-4faf-b6f9-a2a87654d80a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "body = create_timm_body('mobilenetv3_small_050', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4534f06a-9791-49f7-9667-b89599d6ee96",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nf = num_features_model(nn.Sequential(*body.children())); nf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137b322-d501-468d-b516-5b55c4f26df6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "head = create_head(nf, loaders.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830766a1-6fe9-463d-8cbd-349291fc4fc3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(body, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c22d4-0d96-4411-b27a-ff26be703874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "apply_init(model[1], nn.init.kaiming_normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2c2db-80ca-422e-89ac-3a629a1e28d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de90b70-eb2d-451d-95ed-3147203d0353",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_model(model, pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"model: function to load the model, e.g. resnet18\n",
    "        pretrained, progress: to be passed to the model function\n",
    "    \"\"\"\n",
    "    m = model(pretrained=pretrained, progress=progress, **kwargs) # loads standard model\n",
    "    ##m.avgpool = nn.AdaptiveAvgPool2d(output_size=(100,100)) # changes one layer\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8541ff0-7720-48e9-84e6-eb517b0f4ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwilliams/Projects/SEEMAP2023/venv/seemap2023/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/gwilliams/Projects/SEEMAP2023/venv/seemap2023/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_0_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "##learn = vision_learner(loaders, partial(get_model, model=model), loss_func=nn.CrossEntropyLoss(), metrics=accuracy)\n",
    "learn = vision_learner(loaders, squeezenet1_0, loss_func=nn.CrossEntropyLoss(), metrics=accuracy)\n",
    "\n",
    "##learn = Learner(loaders, model, loss_func=LabelSmoothingCrossEntropy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e800df4-7787-48c2-bb6c-94b0e47610cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Sequential (Input shape: 64 x 3 x 28 x 28)\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     64 x 96 x 11 x 11   \n",
       "Conv2d                                    14208      False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 96 x 5 x 5     \n",
       "MaxPool2d                                                      \n",
       "____________________________________________________________________________\n",
       "                     64 x 16 x 5 x 5     \n",
       "Conv2d                                    1552       False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 64 x 5 x 5     \n",
       "Conv2d                                    1088       False     \n",
       "ReLU                                                           \n",
       "Conv2d                                    9280       False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 16 x 5 x 5     \n",
       "Conv2d                                    2064       False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 64 x 5 x 5     \n",
       "Conv2d                                    1088       False     \n",
       "ReLU                                                           \n",
       "Conv2d                                    9280       False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 32 x 5 x 5     \n",
       "Conv2d                                    4128       False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 128 x 5 x 5    \n",
       "Conv2d                                    4224       False     \n",
       "ReLU                                                           \n",
       "Conv2d                                    36992      False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 256 x 2 x 2    \n",
       "MaxPool2d                                                      \n",
       "____________________________________________________________________________\n",
       "                     64 x 32 x 2 x 2     \n",
       "Conv2d                                    8224       False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 128 x 2 x 2    \n",
       "Conv2d                                    4224       False     \n",
       "ReLU                                                           \n",
       "Conv2d                                    36992      False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 48 x 2 x 2     \n",
       "Conv2d                                    12336      False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 192 x 2 x 2    \n",
       "Conv2d                                    9408       False     \n",
       "ReLU                                                           \n",
       "Conv2d                                    83136      False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 48 x 2 x 2     \n",
       "Conv2d                                    18480      False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 192 x 2 x 2    \n",
       "Conv2d                                    9408       False     \n",
       "ReLU                                                           \n",
       "Conv2d                                    83136      False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 64 x 2 x 2     \n",
       "Conv2d                                    24640      False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 256 x 2 x 2    \n",
       "Conv2d                                    16640      False     \n",
       "ReLU                                                           \n",
       "Conv2d                                    147712     False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 512 x 1 x 1    \n",
       "MaxPool2d                                                      \n",
       "____________________________________________________________________________\n",
       "                     64 x 64 x 1 x 1     \n",
       "Conv2d                                    32832      False     \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 256 x 1 x 1    \n",
       "Conv2d                                    16640      False     \n",
       "ReLU                                                           \n",
       "Conv2d                                    147712     False     \n",
       "ReLU                                                           \n",
       "AdaptiveAvgPool2d                                              \n",
       "AdaptiveMaxPool2d                                              \n",
       "____________________________________________________________________________\n",
       "                     64 x 1024           \n",
       "Flatten                                                        \n",
       "BatchNorm1d                               2048       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     64 x 512            \n",
       "Linear                                    524288     True      \n",
       "ReLU                                                           \n",
       "BatchNorm1d                               1024       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     64 x 10             \n",
       "Linear                                    5120       True      \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 1,267,904\n",
       "Total trainable params: 532,480\n",
       "Total non-trainable params: 735,424\n",
       "\n",
       "Optimizer used: <function Adam at 0x7fa48658faf0>\n",
       "Loss function: CrossEntropyLoss()\n",
       "\n",
       "Model frozen up to parameter group #2\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - CastToTensor\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71bd53ea-a01d-4f41-8e4a-2fc485c7b489",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.575967</td>\n",
       "      <td>1.193913</td>\n",
       "      <td>0.867000</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdd76a40-0699-4cc2-8dc8-a9c89bf72668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "pytorch_model = learn.model.eval()\n",
    "print(type(pytorch_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d922253a-1034-45a7-8add-2c832f4e1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pytorch_model, \"data/fastai_mnist.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11d2e129-5928-42ba-8861-5e110b16316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = torch.load(\"data/fastai_mnist.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c6c538-5b03-433f-868d-2278c426cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as tfms\n",
    "def fast2pil(img):\n",
    "  return tfms.ToPILImage()(img.data).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c248d464-fd53-4e79-8fd9-682ed123568c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "<class 'fastai.torch_core.TensorImage'> torch.Size([64, 3, 28, 28])\n",
      "<class 'fastai.torch_core.TensorCategory'> torch.Size([64])\n",
      "TensorCategory(5)\n",
      "<class 'fastai.torch_core.TensorBase'> torch.Size([64, 10])\n",
      "TensorBase([ 2.2706, -2.9245,  0.0870, -1.0103, -0.2144,  2.8094,  2.2570,\n",
      "            -3.4452, -1.9547, -1.0822], grad_fn=<AliasBackward0>)\n",
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'count', 'index']\n",
      "<class 'fastai.torch_core.TensorImage'>\n",
      "<class 'PIL.Image.Image'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACr0lEQVR4nGN88tOEgdqAieomDilDWSjRvHr9ulv3733888vcyDjYxR0uTrJLI6IiJ82eufHgvr8i/DPWrFi7Zyc3G/ulYydIcKmEnKx3UOCapcsYGBi4BfheffxwQunWX2H+czeu+X37cXH/IT1N7fqCYjRdjPiT1My1K/cc2A9hr12zpiAnl5uNfVJXz/+fvz59+oRLF2FDbe3tXCytP336HBkQNHvmTPw+I2zopNkzF6xf8/nR09tXrhFjFhzgjKi+6VOXbN5w6cxZblZ2kkxkwBVRPf397bMnRl65vHrrznMBAeevI1wqwMXr4uB4984d0gxtbmuz9LZzujc76fyiwtbsZbvSHj58+P37dwYGBmlpaV5e3vpJXr5RS/99+rp1yxashmIJ0z0Xz86cM3v5goU3rt84fuDgvr17f335tnb1agYGhicvnydkpL/5+EFVW8vX0yszOvbr+49EGWrm5/n345evr17fuH4Dlwev3b61cseWW/fvd1RWy4tKEDZUWUXl0cOHv3//xmUiBIiJi09fumjR0qUb5y9Ck8IS+3fv3Jm7cIGbhzumFDJ49fJlQW7uX2bGn///ETaUgYFhz4H9ZrY2+A2FgOPHjjfV1aMJYk/81948Tw2LPLrvAB7jVq9fJ6Wtce7Eidy4JKJcysDAwC8tKSYujkt22aqVUtoavLy8qlKymLLYDe1taecW4A+NicKUUlZR2bp/r1tEqKCQYG58koeLK6Ya7Dnq/uWr7bOnx8TF3nn+xExb7927d/IK8gaGhn+ZmVIqXAtqq/YYf1g7a/7BXVVYteMsUJycnf8L8nIL8IvwCxw+dKisomLPgf3fv3//+OZtjG9AenIKVl0EDIWAG4/vL9+w/tqtW58/f+5paVswY2ZPSxse9UQZSh4YOlU0TQwFAAJ2KAXqEKBWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorCategory(5)\n",
      "TensorBase([0.2505, 0.0014, 0.0282, 0.0094, 0.0209, 0.4293, 0.2471, 0.0008,\n",
      "            0.0037, 0.0088], grad_fn=<AliasBackward0>)\n",
      "TensorBase([ 2.2706, -2.9245,  0.0870, -1.0103, -0.2144,  2.8094,  2.2570,\n",
      "            -3.4452, -1.9547, -1.0822], grad_fn=<AliasBackward0>)\n",
      "TensorBase(5)\n"
     ]
    }
   ],
   "source": [
    "b = loaders.one_batch()\n",
    "print(len(b))\n",
    "print( type(b[0]), b[0].shape)\n",
    "print( type(b[1]), b[1].shape)\n",
    "print( b[1][0] )\n",
    "o = pm(b[0])\n",
    "print( type(o), o.shape)\n",
    "print(o[0])\n",
    "print(dir(b))\n",
    "print(type(b[0][0]))\n",
    "if True:\n",
    "    img = fast2pil(b[0][0])\n",
    "    print(type(img))\n",
    "    display(img)\n",
    "print(b[1][0])\n",
    "smax = torch.nn.functional.softmax(o[0])\n",
    "print(smax)\n",
    "print(o[0])\n",
    "m,p = torch.max(smax,-1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f16d79-deb6-4dab-bff9-61c1432cd85c",
   "metadata": {},
   "source": [
    "# We have an issue with prediction based on a loader not part of the origina training??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7eb54099-9265-4f82-8b79-e35bbf171ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = torch.load(\"data/fastai_mnist.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc9e1553-4149-45e9-9e7b-49d2d7883dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gwilliams/.fastai/data/mnist_png\n"
     ]
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "print(path)\n",
    "block2 = DataBlock(\n",
    "        blocks=(ImageBlock, CategoryBlock),\n",
    "        get_items=get_image_files,\n",
    "        splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "        get_y=parent_label)\n",
    "\n",
    "loaders2 = block2.dataloaders(path/\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecbd25c6-b57a-409e-82b3-c6925efa4e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai.torch_core.TensorImage'>\n",
      "<class 'PIL.Image.Image'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAC2ElEQVR4nGN88tOEgdqAiVQNOrq6R29diyzK9fD3o46hfHx8s5cs7O7vO3PqVEFuLi5lLMSbKCQkJGWgcdrwQ4iXj5ORqYeLKxUM/fn/r4q21vUL02N8A/CrJNb7M+fOYREVYmBgEJYUJ6iYWENVtbUcnZ1rq6rK8wupY6iRpbmEiiIDA4OHrcOKZcupYyi7kEBVba2DtQ3b77/EqCds6I3H9+89eczAwLBh5aqnT59SwdCHr19EpadZWlldv3BJUUwCLi6lo2EfHnT66iVLKyvSDL1y77aZi5OcvPyf37+ntnfOnzuPgYHBwNDw1e/vCupqgkJCrVMmTVswh1hDxcTF2Xi49awtLa2sbt64cXLPfhcHRwYGhmWrVi7atiktL1dMTGzOzFkMDAxT583bc2A/mnbsib+souLIrasMDAxXL1/++vL1swePGBgYvH18TtyrtbVvvHr58syeflPVQ4LKDxPLo1zsHQm7tKikJLexhoGBIT0lZduSFc9u32NgYIiIimyfPd3W3o6BgcHL2t7JwurNj69y8vLpsfGYJjBiFn1X7t3Or61+//497x+Gg7t2MzAw6Ojqvvv/W0ZJ0dPV1c3MKj0l1dLKyis2Ijwysru8Oj8HvWRB976yisqBq195+fiO7T/w/dkrBgYGSyur7xzfzUzML2/c9H3TdgYGhiuXGWYvKV24dKnt02enY05juhTd+0lZGbkF+QwMDMXZuQwMDKZmZt85WHRNjFfPna8jowBRo6OrG5OSzMDA8O7du0cPH2Iaiu7S9+/eQRivP7z/+O3ru/+/ddX1GBgYOhqaT5+qYmBgyC4ujM5Xizx7loGBQUtOgdeOF9NQ9DB98fZNRk0FhP350ydePj4GBob46Gh5PqGIhLjErIyjJ06wsLJWlpYJc3L5OLncuH6DsKF8fHxpJYV3nuPLjg9u3qrOLQgNDMKlAEvsUw5IrvhGDR1GhgIAgkIDFwZivZEAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorCategory(6)\n",
      "TensorBase([8.5946e-02, 1.8336e-03, 2.4087e-01, 1.1465e-02, 4.3907e-02,\n",
      "            7.2856e-03, 5.7593e-01, 5.3010e-04, 2.7779e-02, 4.4553e-03],\n",
      "           grad_fn=<AliasBackward0>)\n",
      "TensorBase([ 1.1291e+00, -2.7183e+00,  2.1596e+00, -8.8533e-01,  4.5744e-01,\n",
      "            -1.3387e+00,  3.0314e+00, -3.9593e+00, -3.6687e-04, -1.8305e+00],\n",
      "           grad_fn=<AliasBackward0>)\n",
      "TensorBase(6)\n"
     ]
    }
   ],
   "source": [
    "b = loaders.one_batch()\n",
    "print(type(b[0][0]))\n",
    "img = fast2pil(b[0][0])\n",
    "print(type(img))\n",
    "display(img)\n",
    "print(b[1][0])\n",
    "o = pm(b[0])\n",
    "smax = torch.nn.functional.softmax(o[0])\n",
    "print(smax)\n",
    "print(o[0])\n",
    "m,p = torch.max(smax,-1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4f38b54-50f2-4c0f-9b48-23beb6de0c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai.torch_core.TensorImage'>\n",
      "<class 'PIL.Image.Image'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAAf0lEQVR4nO2UwQ3AIAwDoZMxGkyWMJn7QOVZbCmID/c2pySWSOlyFjMDUEr5jz2SdOiWUg0AAJYxbVISQVprjZfyZD46r5nz4tXpm26UttbCpHz1glSys+3z1Sdy0jmduzN5bf3ee7w0EnxEfnrDaGZkXlifPCgrHaWT1V+28AKVdThKQEElqQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorCategory(1)\n",
      "TensorBase([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], grad_fn=<AliasBackward0>)\n",
      "TensorBase([  151.7767,  -358.5115,   716.9222, -1069.2673, -2446.7822,\n",
      "              968.1915,  -717.2325,  -268.9084,  -985.1187,  -587.1184],\n",
      "           grad_fn=<AliasBackward0>)\n",
      "TensorBase(5)\n"
     ]
    }
   ],
   "source": [
    "bb = loaders2.one_batch()\n",
    "print(type(bb[0][0]))\n",
    "img2 = fast2pil(bb[0][0])\n",
    "print(type(img2))\n",
    "display(img2)\n",
    "print(bb[1][0])\n",
    "oo = pm(bb[0])\n",
    "smaxx = torch.nn.functional.softmax(oo[0])\n",
    "print(smaxx)\n",
    "print(oo[0])\n",
    "mm,pp = torch.max(smaxx,-1)\n",
    "print(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eaa617-6b79-4e3f-abce-7daee5114149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
